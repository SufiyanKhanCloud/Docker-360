===============================
ELK STACK – PHASE 1 & PHASE 2
Beginner Concept Notes
===============================

--------------------------------
PHASE 1: BIG PICTURE OF ELK
--------------------------------

ELK Stack = Elasticsearch + Logstash + Kibana

ELK is used for:
- Centralized logging
- Searching logs
- Monitoring systems
- Debugging issues
- Visualizing data

--------------------------------
ELK COMPONENTS (CORE IDEA)
--------------------------------

1) Logstash
-----------
What it is:
- A log processing tool

Why it is used:
- Collects logs from different sources
- Parses and cleans logs
- Converts messy logs into structured data

What it does:
- Input → Filter → Output

Example:
- Reads system logs
- Extracts fields (time, level, message)
- Sends clean logs to Elasticsearch

2) Elasticsearch
----------------
What it is:
- A search and analytics engine

Why it is used:
- Stores logs
- Indexes logs
- Allows fast searching

Important idea:
- Elasticsearch does NOT store data like a normal database
- It stores data as indexes for fast search

Example:
- Search error logs in last 10 minutes
- Find failed login attempts

3) Kibana
---------
What it is:
- A visualization tool

Why it is used:
- View logs in browser
- Create dashboards
- Analyze trends

Example:
- Graph of errors per hour
- Dashboard showing system health

--------------------------------
PHASE 2: ELK BEST PRACTICES
--------------------------------

1) Use Docker / Docker Compose
------------------------------
What it is:
- Running ELK inside containers

Why it is used:
- Easy setup
- No dependency issues
- Same environment everywhere
- Easy start and stop

Connection:
- Docker runs Elasticsearch
- Docker runs Logstash
- Docker runs Kibana
- Docker Compose connects them via network

2) Use Volumes for Elasticsearch Data
-------------------------------------
What it is:
- Persistent storage outside container

Why it is used:
- Containers can be deleted
- Volumes keep data safe

Without volume:
- Container removed → data lost

With volume:
- Container removed → data remains

Connection:
- Elasticsearch stores indexes in volume

IMPORTANT:
- Never casually delete volumes

3) Do NOT Send Raw Logs Directly to Elasticsearch
-------------------------------------------------
What it is:
- Logs should go through Logstash first

Why:
- Raw logs are messy
- Elasticsearch expects structured data

Logstash does:
- Parsing
- Cleaning
- Adding fields
- Formatting data

Correct flow:
Logs → Logstash → Elasticsearch

4) Use Structured Logs (JSON)
-----------------------------
What it is:
- Logs written as key-value pairs

Why:
- Easier searching
- Faster indexing
- Better dashboards

Example:
level=ERROR
service=auth
message=Login failed

Connection:
- Logstash understands structure
- Elasticsearch indexes fields
- Kibana visualizes fields
